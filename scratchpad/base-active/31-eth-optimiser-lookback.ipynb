{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Memecoin active trading strategy on Base\n",
    "\n",
    "- Gaussian process optimiser\n",
    "- Discover parameter combinations\n",
    "- Add `min_volatility_threshold` search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Set up\n",
    "\n",
    "Set up Trading Strategy data client.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-19T08:48:16.546848Z",
     "start_time": "2024-07-19T08:48:16.464970Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "from tradingstrategy.client import Client\n",
    "from tradeexecutor.utils.notebook import setup_charting_and_output, OutputMode\n",
    "\n",
    "client = Client.create_jupyter_client()\n",
    "\n",
    "# Set up drawing charts in interactive vector output mode.\n",
    "# This is slower. See the alternative commented option below.\n",
    "# setup_charting_and_output(OutputMode.interactive)\n",
    "\n",
    "# Set up rendering static PNG images.\n",
    "# This is much faster but disables zoom on any chart.\n",
    "setup_charting_and_output(OutputMode.static, image_format=\"png\", width=1500, height=1000)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Parameters\n",
    "\n",
    "- Collection of parameters used in the calculations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-19T08:48:16.558597Z",
     "start_time": "2024-07-19T08:48:16.556375Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "import pandas as pd\n",
    "from pandas.tseries.offsets import MonthBegin\n",
    "\n",
    "from skopt.space import Integer, Real, Categorical\n",
    "\n",
    "from tradingstrategy.chain import ChainId\n",
    "from tradingstrategy.timebucket import TimeBucket\n",
    "from tradeexecutor.strategy.cycle import CycleDuration\n",
    "from tradeexecutor.strategy.parameters import StrategyParameters\n",
    "from tradeexecutor.strategy.default_routing_options import TradeRouting\n",
    "\n",
    "\n",
    "class Parameters:\n",
    "\n",
    "    id = \"31-eth-optimiser-periods\"\n",
    "\n",
    "    # We trade 1h candle\n",
    "    candle_time_bucket = TimeBucket.h1\n",
    "    cycle_duration = CycleDuration.cycle_4h\n",
    "    \n",
    "    # Coingecko categories to include\n",
    "    #\n",
    "    # See list here: TODO\n",
    "    #\n",
    "    chain_id = ChainId.ethereum\n",
    "    categories = {\"Meme\"}\n",
    "    exchanges = {\"uniswap-v2\", \"uniswap-v3\"}\n",
    "    \n",
    "    #\n",
    "    # Basket construction and rebalance parameters\n",
    "    #\n",
    "    min_asset_universe = 5  # How many assets we need in the asset universe to start running the index\n",
    "\n",
    "    # How many assets our basket can hold once\n",
    "    max_assets_in_portfolio = 30\n",
    "    allocation = 0.95  # Allocate all cash to volatile pairs\n",
    "    # min_rebalance_trade_threshold_pct = 0.05  # % of portfolio composition must change before triggering rebalacne\n",
    "    individual_rebalance_min_threshold_usd = 75.0  # Don't make buys less than this amount    \n",
    "    per_position_cap_of_pool = 0.01  # Never own more than % of the lit liquidity of the trading pool\n",
    "    max_concentration = 0.33 # How large % can one asset be in a portfolio once\n",
    "    min_portfolio_weight = 0.0050  # Close position / do not open if weight is less than 50 BPS\n",
    "\n",
    "    min_signal_threshold = Categorical([\n",
    "        #0.025,\n",
    "        0.05,\n",
    "        #0.075,\n",
    "        #0.10,\n",
    "        0.125,\n",
    "        #0.150,\n",
    "        0.175,\n",
    "    ])\n",
    "    \n",
    "    #\n",
    "    # Inclusion criteria parameters:\n",
    "    # - We set the length of various indicators used in the inclusion criteria\n",
    "    # - We set minimum thresholds neede to be included in the index to filter out illiquid pairs\n",
    "    #\n",
    "\n",
    "    # For the length of trailing sharpe used in inclusion criteria\n",
    "    trailing_sharpe_bars = pd.Timedelta(\"14d\") // candle_time_bucket.to_timedelta()  # How many bars to use in trailing sharpe indicator\n",
    "    # How many bars to use in volatility indicator\n",
    "    rebalance_volalitity_bars = Categorical([\n",
    "        pd.Timedelta(\"4d\") // candle_time_bucket.to_timedelta(),\n",
    "        pd.Timedelta(\"2d\") // candle_time_bucket.to_timedelta(),\n",
    "        #pd.Timedelta(\"1d\") // candle_time_bucket.to_timedelta(),\n",
    "        pd.Timedelta(\"12h\") // candle_time_bucket.to_timedelta(),\n",
    "        #pd.Timedelta(\"8h\") // candle_time_bucket.to_timedelta(),\n",
    "    ])\n",
    "    returns_bars = Categorical([\n",
    "        #pd.Timedelta(\"14d\") // candle_time_bucket.to_timedelta(),\n",
    "        pd.Timedelta(\"10d\") // candle_time_bucket.to_timedelta(),\n",
    "        pd.Timedelta(\"7d\") // candle_time_bucket.to_timedelta(),\n",
    "        #pd.Timedelta(\"220h\") // candle_time_bucket.to_timedelta(),\n",
    "        #pd.Timedelta(\"140h\") // candle_time_bucket.to_timedelta(),\n",
    "        #pd.Timedelta(\"100h\") // candle_time_bucket.to_timedelta(),\n",
    "        pd.Timedelta(\"3d\") // candle_time_bucket.to_timedelta(),\n",
    "    ])\n",
    "    rolling_volume_bars = pd.Timedelta(\"7d\") // candle_time_bucket.to_timedelta()\n",
    "    rolling_liquidity_bars = pd.Timedelta(\"7d\") // candle_time_bucket.to_timedelta()\n",
    "    ewm_span = 200  # How many bars to use in exponential moving average for trailing sharpe smoothing\n",
    "    tvl_ewm_span = 200  # How many bars to use in EWM smoothing of TVLs\n",
    "    min_volume = 50_000   # USD\n",
    "    min_liquidity = 200_000  # USD\n",
    "    min_tvl = 25_000  # USD\n",
    "    min_token_sniffer_score = 30  # Scam filter\n",
    "\n",
    "    #\n",
    "    # Credit integration\n",
    "    #\n",
    "    use_aave = False\n",
    "    \n",
    "    #\n",
    "    # Backtesting only\n",
    "    #\n",
    "    backtest_start = datetime.datetime(2023, 5, 3)\n",
    "    backtest_end = datetime.datetime(2025, 1, 11)\n",
    "    initial_cash = 10_000\n",
    "    min_trade_count = 50  # Used in optimiser/grid search to filter out invvalid results\n",
    "\n",
    "    # How do we build the time-local parameters\n",
    "    optimiser_lookback_period = datetime.timedelta(days=93)\n",
    "    optimiser_checkpoint_period = MonthBegin(1)  # Month start\n",
    "\n",
    "    #\n",
    "    # Live only\n",
    "    #\n",
    "    routing = TradeRouting.default\n",
    "    required_history_period = datetime.timedelta(days=2*14 + 1)\n",
    "    slippage_tolerance = 0.0060  # 0.6% \n",
    "    assummed_liquidity_when_data_missings = 10_000\n",
    "    \n",
    "\n",
    "parameters = StrategyParameters.from_class(Parameters)  # Convert to AttributedDict to easier typing with dot notation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Trading pairs and market data\n",
    "\n",
    "- This creates the strategy universe containing pair metadata and their prices\n",
    "- The universe is \"masked\" by simply selecting pairs on the predefined pairs list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-19T08:48:18.342247Z",
     "start_time": "2024-07-19T08:48:16.556521Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "from tradingstrategy.alternative_data.coingecko import CoingeckoUniverse, categorise_pairs\n",
    "from tradingstrategy.chain import ChainId\n",
    "from tradingstrategy.client import Client\n",
    "from tradingstrategy.pair import PandasPairUniverse\n",
    "from tradingstrategy.utils.token_filter import deduplicate_pairs_by_volume\n",
    "from tradingstrategy.client import Client\n",
    "from tradingstrategy.client import Client\n",
    "\n",
    "from tradeexecutor.strategy.trading_strategy_universe import TradingStrategyUniverse\n",
    "from tradeexecutor.strategy.execution_context import ExecutionContext, notebook_execution_context\n",
    "from tradeexecutor.strategy.universe_model import UniverseOptions\n",
    "from tradeexecutor.strategy.trading_strategy_universe import TradingStrategyUniverse, load_partial_data, OHLCVCandleType\n",
    "from tradeexecutor.strategy.execution_context import ExecutionContext, notebook_execution_context\n",
    "from tradeexecutor.strategy.universe_model import UniverseOptions\n",
    "from tradingstrategy.utils.token_extra_data import filter_scams\n",
    "from tradingstrategy.lending import LendingProtocolType\n",
    "from tradeexecutor.analysis.pair import display_strategy_universe\n",
    "\n",
    "from eth_defi.token import USDC_NATIVE_TOKEN\n",
    "\n",
    "\n",
    "\n",
    "# We define our main trading universe,\n",
    "# and then Ethereum mainnet as a validation set\n",
    "if Parameters.chain_id == ChainId.base:\n",
    "    SUPPORTING_PAIRS = [\n",
    "        (ChainId.base, \"uniswap-v2\", \"WETH\", \"USDC\", 0.0030),  \n",
    "        (ChainId.base, \"uniswap-v3\", \"cbBTC\", \"WETH\", 0.0030),    # Only trading since October\n",
    "    ]\n",
    "\n",
    "    EXAMINED_ASSETS = [\n",
    "        \"AIXBT\", \n",
    "        \"PEPE\",\n",
    "        \"KEYCAT\",\n",
    "        \"WETH\",\n",
    "    ]\n",
    "\n",
    "    VOL_PAIR = (ChainId.base, \"uniswap-v2\", \"WETH\", \"USDC\", 0.0030)\n",
    "\n",
    "elif Parameters.chain_id == ChainId.ethereum:\n",
    "    SUPPORTING_PAIRS = [\n",
    "        (ChainId.ethereum, \"uniswap-v2\", \"WETH\", \"USDC\", 0.0030),  \n",
    "        (ChainId.ethereum, \"uniswap-v3\", \"WBTC\", \"USDC\", 0.0030),    # Only trading since October\n",
    "    ]\n",
    "\n",
    "    EXAMINED_ASSETS = [\n",
    "        \"PEPE\",\n",
    "        \"Neiro\",\n",
    "        \"APU\",\n",
    "        \"WETH\",\n",
    "    ]\n",
    "    VOL_PAIR = (ChainId.ethereum, \"uniswap-v2\", \"WETH\", \"USDC\", 0.0030)\n",
    "\n",
    "else:\n",
    "    raise NotImplementedError(\"Chain not supported\")\n",
    "\n",
    "\n",
    "#: Needed for USDC credit\n",
    "LENDING_RESERVES  = [\n",
    "    (Parameters.chain_id, LendingProtocolType.aave_v3, \"USDC\"),\n",
    "]\n",
    "\n",
    "\n",
    "def create_trading_universe(\n",
    "    timestamp: datetime.datetime,\n",
    "    client: Client,\n",
    "    execution_context: ExecutionContext,\n",
    "    universe_options: UniverseOptions,\n",
    ") -> TradingStrategyUniverse:\n",
    "    \"\"\"Create the trading universe.\n",
    "\n",
    "    - Load Trading Strategy full pairs dataset\n",
    "\n",
    "    - Load built-in Coingecko top 1000 dataset\n",
    "    \n",
    "    - Get all DEX tokens for a certain Coigecko category\n",
    "\n",
    "    - Load OHCLV data for these pairs\n",
    "\n",
    "    - Load also BTC and ETH price data to be used as a benchmark\n",
    "    \"\"\"\n",
    "\n",
    "    print(f\"Loading backtesting data for {universe_options.start_at} - {universe_options.end_at}\")\n",
    "\n",
    "    chain_id = Parameters.chain_id\n",
    "    categories = Parameters.categories\n",
    "\n",
    "    coingecko_universe = CoingeckoUniverse.load()\n",
    "    print(\"Coingecko universe is\", coingecko_universe)\n",
    "\n",
    "    exchange_universe = client.fetch_exchange_universe()\n",
    "    pairs_df = client.fetch_pair_universe().to_pandas()\n",
    "\n",
    "    # Drop other chains to make the dataset smaller to work with\n",
    "    chain_mask = pairs_df[\"chain_id\"] == Parameters.chain_id.value\n",
    "    pairs_df = pairs_df[chain_mask]\n",
    "\n",
    "    # Pull out our benchmark pairs ids.\n",
    "    # We need to construct pair universe object for the symbolic lookup.\n",
    "    pair_universe = PandasPairUniverse(pairs_df, exchange_universe=exchange_universe)\n",
    "    benchmark_pair_ids = [pair_universe.get_pair_by_human_description(desc).pair_id for desc in SUPPORTING_PAIRS]\n",
    "\n",
    "    # Assign categories to all pairs\n",
    "    category_df = categorise_pairs(coingecko_universe, pairs_df)\n",
    "\n",
    "    # Get all trading pairs that are memecoin, across all coingecko data\n",
    "    mask = category_df[\"category\"].isin(categories)\n",
    "    category_pair_ids = category_df[mask][\"pair_id\"]\n",
    "\n",
    "    our_pair_ids = list(category_pair_ids) + benchmark_pair_ids\n",
    "\n",
    "    # From these pair ids, see what trading pairs we have on Ethereum mainnet\n",
    "    pairs_df = pairs_df[pairs_df[\"pair_id\"].isin(our_pair_ids)]\n",
    "\n",
    "    # Limit by DEX\n",
    "    pairs_df = pairs_df[pairs_df[\"exchange_slug\"].isin(Parameters.exchanges)]\n",
    "\n",
    "    # Never deduplicate supporrting pars\n",
    "    supporting_pairs_df = pairs_df[pairs_df[\"pair_id\"].isin(benchmark_pair_ids)]\n",
    "    \n",
    "    # Deduplicate trading pairs - Choose the best pair with the best volume\n",
    "    deduplicated_df = deduplicate_pairs_by_volume(pairs_df)\n",
    "    pairs_df = pd.concat([deduplicated_df, supporting_pairs_df]).drop_duplicates(subset='pair_id', keep='first')\n",
    "\n",
    "    print(\n",
    "        f\"Total {len(pairs_df)} pairs to trade on {chain_id.name} for categories {categories}\",        \n",
    "    )\n",
    "\n",
    "    # Scam filter using TokenSniffer\n",
    "    pairs_df = filter_scams(pairs_df, client, min_token_sniffer_score=Parameters.min_token_sniffer_score)\n",
    "    pairs_df = pairs_df.sort_values(\"volume\", ascending=False)\n",
    "\n",
    "    uni_v2 = pairs_df.loc[pairs_df[\"exchange_slug\"] == \"uniswap-v2\"]\n",
    "    uni_v3 = pairs_df.loc[pairs_df[\"exchange_slug\"] == \"uniswap-v3\"]\n",
    "    print(f\"Pairs on Uniswap v2: {len(uni_v2)}, Uniswap v3: {len(uni_v3)}\")\n",
    "\n",
    "    dataset = load_partial_data(\n",
    "        client=client,\n",
    "        time_bucket=Parameters.candle_time_bucket,\n",
    "        pairs=pairs_df,\n",
    "        execution_context=execution_context,\n",
    "        universe_options=universe_options,\n",
    "        liquidity=True,\n",
    "        liquidity_time_bucket=TimeBucket.d1,  \n",
    "        liquidity_query_type=OHLCVCandleType.tvl_v2,\n",
    "    )\n",
    "\n",
    "    reserve_asset = USDC_NATIVE_TOKEN[chain_id.value]\n",
    "\n",
    "    strategy_universe = TradingStrategyUniverse.create_from_dataset(\n",
    "        dataset,\n",
    "        reserve_asset=reserve_asset,\n",
    "        forward_fill=True,  # We got very gappy data from low liquid DEX coins\n",
    "    )\n",
    "\n",
    "\n",
    "    # Tag benchmark/routing pairs tokens so they can be separated from the rest of the tokens\n",
    "    # for the index construction.\n",
    "    strategy_universe.warm_up_data()\n",
    "    for pair_id in benchmark_pair_ids:\n",
    "        pair = strategy_universe.get_pair_by_id(pair_id)\n",
    "        pair.other_data[\"benchmark\"] = True\n",
    "\n",
    "    return strategy_universe\n",
    "\n",
    "    \n",
    "strategy_universe = create_trading_universe(\n",
    "    None,\n",
    "    client,\n",
    "    notebook_execution_context,\n",
    "    UniverseOptions.from_strategy_parameters_class(Parameters, notebook_execution_context)\n",
    ")\n",
    "\n",
    "display_strategy_universe(strategy_universe)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Asset to trading pair map\n",
    "\n",
    "- Build a helper map\n",
    "- Because we are operating on trading pairs, not on tokens, which are the base asset of a trading pair, we set up \n",
    "  this map to easily look up the selected trading pair by its symbol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tradingstrategy.types import TokenSymbol\n",
    "from tradeexecutor.state.identifier import TradingPairIdentifier\n",
    "\n",
    "# Create base token symbol to pair map to help later\n",
    "# Token\n",
    "token_map: dict[TokenSymbol, TradingPairIdentifier] = {p.base.token_symbol: p for p in strategy_universe.iterate_pairs()}\n",
    "\n",
    "# print(f\"Pair count is {strategy_universe.get_pair_count()}\")\n",
    "# for symbol, pair in token_map.items():\n",
    "#    print(f\"{symbol} - #{pair.internal_id}\")\n",
    "\n",
    "# Tokens part of benchmark data, but not the strategy\n",
    "benchmark_pair_ids = [p.internal_id for p in strategy_universe.iterate_pairs() if p.other_data.get(\"benchmark\") is True]\n",
    "category_pair_ids = [p.internal_id for p in strategy_universe.iterate_pairs() if p.other_data.get(\"benchmark\") is not True]\n",
    "\n",
    "print(f\"Token map is {len(token_map)} assets\")\n",
    "print(\"Category trading pairs\", len(category_pair_ids))\n",
    "print(\"Benchmark trading pairs\", len(benchmark_pair_ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Indicators\n",
    "\n",
    "- Precalculate indicators used by the strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from IPython.display import HTML\n",
    "\n",
    "from tradeexecutor.strategy.execution_context import ExecutionContext\n",
    "from tradeexecutor.strategy.pandas_trader.indicator import IndicatorSet, IndicatorSource\n",
    "from tradeexecutor.strategy.parameters import StrategyParameters\n",
    "from tradeexecutor.strategy.trading_strategy_universe import TradingStrategyUniverse\n",
    "from tradeexecutor.strategy.pandas_trader.indicator import calculate_and_load_indicators_inline\n",
    "from tradeexecutor.strategy.pandas_trader.indicator import IndicatorDependencyResolver\n",
    "from tradeexecutor.state.types import USDollarAmount\n",
    "from tradeexecutor.strategy.pandas_trader.indicator_decorator import IndicatorRegistry\n",
    "from tradeexecutor.analysis.indicator import display_indicators\n",
    "\n",
    "\n",
    "indicators = IndicatorRegistry()\n",
    "\n",
    "\n",
    "@indicators.define()\n",
    "def trailing_sharpe(\n",
    "    close: pd.Series, \n",
    "    trailing_sharpe_bars: int\n",
    ") -> pd.Series:\n",
    "    \"\"\"Calculate trailing 30d or so returns / standard deviation.\n",
    "\n",
    "    :param length:\n",
    "        Trailing period. \n",
    "    \n",
    "    :return:\n",
    "        Rolling cumulative returns / rolling standard deviation\n",
    "\n",
    "        Note that this trailing sharpe is not annualised.\n",
    "    \"\"\"\n",
    "    ann_factor = pd.Timedelta(days=365) / Parameters.candle_time_bucket.to_pandas_timedelta()\n",
    "    returns = close.pct_change()\n",
    "    mean_returns = returns.rolling(window=trailing_sharpe_bars).mean()    \n",
    "    vol = returns.rolling(window=trailing_sharpe_bars).std()\n",
    "    return mean_returns / vol * np.sqrt(ann_factor)\n",
    "\n",
    "\n",
    "@indicators.define(dependencies=(trailing_sharpe,), source=IndicatorSource.dependencies_only_per_pair)\n",
    "def trailing_sharpe_ewm(\n",
    "    trailing_sharpe_bars: int,\n",
    "    ewm_span: float,\n",
    "    pair: TradingPairIdentifier,\n",
    "    dependency_resolver: IndicatorDependencyResolver,\n",
    ") -> pd.Series:\n",
    "    \"\"\"Expontentially weighted moving average for Sharpe.\n",
    "\n",
    "    :param ewm_span:\n",
    "        How many bars to consider in the EVM\n",
    "\n",
    "    \"\"\"\n",
    "    trailing_sharpe = dependency_resolver.get_indicator_data(\n",
    "        \"trailing_sharpe\",\n",
    "        pair=pair,\n",
    "        parameters={\"trailing_sharpe_bars\": trailing_sharpe_bars},\n",
    "    )    \n",
    "    ewm = trailing_sharpe.ewm(span=ewm_span)\n",
    "    return ewm.mean()\n",
    "\n",
    "\n",
    "@indicators.define()\n",
    "def volatility(close: pd.Series, rebalance_volalitity_bars: int) -> pd.Series:\n",
    "    \"\"\"Calculate the rolling volatility for rebalancing the index for each decision cycle.\"\"\"\n",
    "    price_diff = close.pct_change()\n",
    "    rolling_std = price_diff.rolling(window=rebalance_volalitity_bars).std()\n",
    "    return rolling_std\n",
    "\n",
    "\n",
    "\n",
    "@indicators.define()\n",
    "def signed_volatility(close: pd.Series, rebalance_volalitity_bars: int) -> pd.Series:\n",
    "    \"\"\"Volatility with returns sign (profit or loss).\"\"\"\n",
    "    price_diff = close.pct_change()\n",
    "    rolling_std = price_diff.rolling(window=rebalance_volalitity_bars).std()\n",
    "\n",
    "    returns =  close \\\n",
    "        .rolling(window=rebalance_volalitity_bars) \\\n",
    "        .agg(lambda x: x.iloc[-1] - x.iloc[0])\n",
    "    returns_sign = returns.apply(np.sign)\n",
    "    return rolling_std * returns_sign\n",
    "\n",
    "\n",
    "@indicators.define()\n",
    "def rolling_returns(\n",
    "    close: pd.Series, \n",
    "    returns_bars: int    \n",
    ") -> pd.Series:\n",
    "    \"\"\"Rolling returns for the signal period.\"\"\"\n",
    "\n",
    "    def _agg_func(window: pd.Series) -> float:\n",
    "\n",
    "        if len(window) < 2:\n",
    "            return 0\n",
    "\n",
    "        try:\n",
    "            return (window.iloc[-1] - window.iloc[0]) / window.iloc[0]\n",
    "        except Exception as e:\n",
    "            raise # Drop into the debugger here\n",
    "\n",
    "    returns = close.rolling(window=returns_bars).agg(_agg_func)\n",
    "    return returns\n",
    "\n",
    "\n",
    "@indicators.define(dependencies=[rolling_returns])\n",
    "def volatility_returns_indicator(\n",
    "    close: pd.Series, \n",
    "    rebalance_volalitity_bars: int,\n",
    "    returns_bars: int,\n",
    "    pair: TradingPairIdentifier,\n",
    "    dependency_resolver: IndicatorDependencyResolver\n",
    ") -> pd.Series:\n",
    "    \"\"\"Figure out some indicator to predict the price.\"\"\"\n",
    "\n",
    "    returns = dependency_resolver.get_indicator_data(\n",
    "        rolling_returns,\n",
    "        parameters={\"returns_bars\": returns_bars},\n",
    "        pair=pair,\n",
    "    )\n",
    "\n",
    "    price_diff = close.pct_change()\n",
    "    rolling_std = price_diff.rolling(window=rebalance_volalitity_bars).std()\n",
    "    return rolling_std * returns\n",
    "\n",
    "\n",
    "\n",
    "@indicators.define()\n",
    "def volatility_ewm(close: pd.Series, rebalance_volalitity_bars: int) -> pd.Series:\n",
    "    \"\"\"Calculate the rolling volatility for rebalancing the index for each decision cycle.\"\"\"\n",
    "    # We are operating on 1h candles, 14d window\n",
    "    price_diff = close.pct_change()\n",
    "    rolling_std = price_diff.rolling(window=rebalance_volalitity_bars).std()\n",
    "    ewm = rolling_std.ewm(span=14*8)\n",
    "    return ewm.mean()   \n",
    "\n",
    "\n",
    "@indicators.define()\n",
    "def mean_returns(close: pd.Series, rebalance_volalitity_bars: int) -> pd.Series:\n",
    "    # Descripton: TODO\n",
    "    returns = close.pct_change()\n",
    "    mean_returns = returns.rolling(window=rebalance_volalitity_bars).mean()    \n",
    "    return mean_returns\n",
    "\n",
    "\n",
    "@indicators.define()\n",
    "def rolling_cumulative_volume(volume: pd.Series, rolling_volume_bars: int) -> pd.Series:\n",
    "    \"\"\"Calculate rolling volume of the pair.\n",
    "    \n",
    "    - Used in inclusion criteria\n",
    "    \"\"\"\n",
    "    rolling_volume = volume.rolling(window=rolling_volume_bars).sum()\n",
    "    return rolling_volume\n",
    "\n",
    "\n",
    "@indicators.define()\n",
    "def rolling_liquidity_avg(close: pd.Series, rolling_volume_bars: int) -> pd.Series:\n",
    "    \"\"\"Calculate rolling liquidity average\n",
    "\n",
    "    - This is either TVL or XY liquidity (one sided) depending on the trading pair DEX type\n",
    "    \n",
    "    - Used in inclusion criteria\n",
    "    \"\"\"\n",
    "    rolling_liquidity_close = close.rolling(window=rolling_volume_bars).mean()\n",
    "    return rolling_liquidity_close\n",
    "\n",
    "    \n",
    "@indicators.define(dependencies=(rolling_cumulative_volume,), source=IndicatorSource.strategy_universe)\n",
    "def volume_inclusion_criteria(   \n",
    "    strategy_universe: TradingStrategyUniverse, \n",
    "    min_volume: USDollarAmount,\n",
    "    rolling_volume_bars: int,\n",
    "    dependency_resolver: IndicatorDependencyResolver,\n",
    ") -> pd.Series:\n",
    "    \"\"\"Calculate pair volume inclusion criteria.\n",
    "\n",
    "    - Avoid including illiquid / broken pairs in the set: Pair is included when it has enough volume \n",
    "\n",
    "    TODO: Add liquidity check later\n",
    "\n",
    "    :return:\n",
    "        Series where each timestamp is a list of pair ids meeting the criteria at that timestamp\n",
    "    \"\"\"\n",
    "\n",
    "    series = dependency_resolver.get_indicator_data_pairs_combined(\n",
    "        rolling_cumulative_volume,\n",
    "        parameters={\"rolling_volume_bars\": rolling_volume_bars},\n",
    "    )\n",
    "\n",
    "    # Get mask for days when the rolling volume meets out criteria\n",
    "    mask = series >= min_volume\n",
    "    mask_true_values_only = mask[mask == True]\n",
    "\n",
    "    # Turn to a series of lists\n",
    "    series = mask_true_values_only.groupby(level='timestamp').apply(lambda x: x.index.get_level_values('pair_id').tolist())\n",
    "    return series\n",
    "\n",
    "\n",
    "@indicators.define(dependencies=(volatility_ewm,), source=IndicatorSource.strategy_universe)\n",
    "def volatility_inclusion_criteria(   \n",
    "    strategy_universe: TradingStrategyUniverse, \n",
    "    rebalance_volalitity_bars: int,\n",
    "    dependency_resolver: IndicatorDependencyResolver,\n",
    ") -> pd.Series:\n",
    "    \"\"\"Calculate volatility inclusion criteria.\n",
    "\n",
    "    - Include pairs that are above our threshold signal\n",
    "\n",
    "    :return:\n",
    "        Series where each timestamp is a list of pair ids meeting the criteria at that timestamp\n",
    "    \"\"\"\n",
    "    \n",
    "    series = dependency_resolver.get_indicator_data_pairs_combined(\n",
    "       volatility_ewm,\n",
    "        parameters={\"rebalance_volalitity_bars\": rebalance_volalitity_bars},\n",
    "    )\n",
    "\n",
    "    threshold_pair = strategy_universe.get_pair_by_human_description(VOL_PAIR)\n",
    "    assert threshold_pair\n",
    "    threshold_signal = dependency_resolver.get_indicator_data(\n",
    "        volatility_ewm,\n",
    "        pair=threshold_pair,\n",
    "        parameters={\"rebalance_volalitity_bars\": rebalance_volalitity_bars},\n",
    "    )\n",
    "\n",
    "    assert threshold_signal is not None, \"No threshold volatility signal for: {threshold_pair}\"\n",
    "\n",
    "    # Get mask for days when the rolling volume meets out criteria,\n",
    "    # and max out the threshold signal if there is\n",
    "    # mask = filtered_series >= threshold_signal\n",
    "    df = series.reset_index()\n",
    "    df2 = df.merge(threshold_signal, on=[\"timestamp\"], suffixes=('_pair', '_reference'))\n",
    "\n",
    "    #         pair_id           timestamp  value_pair  value_reference\n",
    "    # 0       4569519 2024-02-13 16:00:00    0.097836              NaN\n",
    "    # 1       4569519 2024-02-13 17:00:00    0.097773              NaN\n",
    "\n",
    "    high_volatility_rows = df2[df2[\"value_pair\"] >= df2[\"value_reference\"]]\n",
    "\n",
    "    def _get_pair_ids_as_list(rows):\n",
    "        return rows[\"pair_id\"].tolist()\n",
    "    \n",
    "    # Turn to a series of lists\n",
    "    series = high_volatility_rows.groupby(by=['timestamp']).apply(_get_pair_ids_as_list)\n",
    "    assert isinstance(series, pd.Series)\n",
    "    return series\n",
    "\n",
    "\n",
    "@indicators.define(source=IndicatorSource.tvl)\n",
    "def tvl(\n",
    "    close: pd.Series,\n",
    ") -> pd.Series:\n",
    "    \"\"\"Get TVL series for a pair.\n",
    "\n",
    "    - Because TVL data is 1d and we use 1h everywhere else, we need to forward fill\n",
    "\n",
    "    - Use previous hourly close as the value\n",
    "    \"\"\"\n",
    "    return close.resample(\"1h\").ffill()\n",
    "\n",
    "\n",
    "@indicators.define(dependencies=(tvl,), source=IndicatorSource.dependencies_only_per_pair)\n",
    "def tvl_ewm(\n",
    "    pair: TradingPairIdentifier,\n",
    "    tvl_ewm_span: float,\n",
    "    dependency_resolver: IndicatorDependencyResolver,\n",
    ") -> pd.Series:\n",
    "    \"\"\"Get smoothed TVL series for a pair.\n",
    "\n",
    "    - Interpretation: If you set span=5, for example, the ewm function will compute an exponential moving average where the weight of the most recent observation is about 33.3% (since α=2/(5+1)≈0.333) and this weight decreases exponentially for older observations.\n",
    "\n",
    "    - We forward fill gaps, so there is no missing data in decide_trades()    \n",
    "\n",
    "    - Currently unused in the strategy itself\n",
    "    \"\"\"\n",
    "    tvl_ff = dependency_resolver.get_indicator_data(\n",
    "        tvl,\n",
    "        pair=pair,\n",
    "    )    \n",
    "    return tvl_ff.ewm(span=tvl_ewm_span).mean()\n",
    "\n",
    "\n",
    "@indicators.define(dependencies=(tvl,), source=IndicatorSource.dependencies_only_universe)\n",
    "def tvl_inclusion_criteria(   \n",
    "    min_tvl: USDollarAmount,\n",
    "    dependency_resolver: IndicatorDependencyResolver,\n",
    ") -> pd.Series:\n",
    "    \"\"\"The pair must have min XX,XXX USD one-sided TVL to be included.\n",
    "\n",
    "    - If the Uniswap pool does not have enough ETH or USDC deposited, skip the pair as a scam\n",
    "\n",
    "    :return:\n",
    "        Series where each timestamp is a list of pair ids meeting the criteria at that timestamp\n",
    "    \"\"\"\n",
    "    \n",
    "    series = dependency_resolver.get_indicator_data_pairs_combined(tvl)\n",
    "    mask = series >= min_tvl\n",
    "    # Turn to a series of lists\n",
    "    mask_true_values_only = mask[mask == True]\n",
    "    series = mask_true_values_only.groupby(level='timestamp').apply(lambda x: x.index.get_level_values('pair_id').tolist())\n",
    "    return series\n",
    "\n",
    "\n",
    "\n",
    "@indicators.define(\n",
    "    source=IndicatorSource.strategy_universe\n",
    ")\n",
    "def trading_availability_criteria(\n",
    "    strategy_universe: TradingStrategyUniverse,\n",
    ") -> pd.Series:\n",
    "    \"\"\"Is pair tradeable at each hour.\n",
    "\n",
    "    - The pair has a price candle at that\n",
    "    - Mitigates very corner case issues that TVL/liquidity data is per-day whileas price data is natively per 1h\n",
    "      and the strategy inclusion criteria may include pair too early hour based on TVL only,\n",
    "      leading to a failed attempt to rebalance in a backtest\n",
    "    - Only relevant for backtesting issues if we make an unlucky trade on the starting date\n",
    "      of trading pair listing\n",
    "\n",
    "    :return:\n",
    "        Series with with index (timestamp) and values (list of pair ids trading at that hour)\n",
    "    \"\"\"\n",
    "    # Trading pair availability is defined if there is a open candle in the index for it.\n",
    "    # Because candle data is forward filled, we should not have any gaps in the index.\n",
    "    candle_series = strategy_universe.data_universe.candles.df[\"open\"]\n",
    "    pairs_per_timestamp = candle_series.groupby(level='timestamp').apply(lambda x: x.index.get_level_values('pair_id').tolist())\n",
    "    return pairs_per_timestamp\n",
    "\n",
    "\n",
    "@indicators.define(\n",
    "    dependencies=[\n",
    "        volume_inclusion_criteria,\n",
    "        volatility_inclusion_criteria,\n",
    "        tvl_inclusion_criteria,\n",
    "        trading_availability_criteria\n",
    "    ],\n",
    "    source=IndicatorSource.strategy_universe\n",
    ")\n",
    "def inclusion_criteria(\n",
    "    strategy_universe: TradingStrategyUniverse,\n",
    "    min_volume: USDollarAmount,\n",
    "    rolling_volume_bars: int,\n",
    "    rebalance_volalitity_bars: int,\n",
    "    min_tvl: USDollarAmount,\n",
    "    dependency_resolver: IndicatorDependencyResolver\n",
    ") -> pd.Series:\n",
    "    \"\"\"Pairs meeting all of our inclusion criteria.\n",
    "\n",
    "    - Give the tradeable pair set for each timestamp\n",
    "\n",
    "    :return:\n",
    "        Series where index is timestamp and each cell is a list of pair ids matching our inclusion criteria at that moment\n",
    "    \"\"\"\n",
    "\n",
    "    assert type(rebalance_volalitity_bars) == int, f\"Got: {rebalance_volalitity_bars}\"\n",
    "\n",
    "    # Filter out benchmark pairs like WETH in the tradeable pair set\n",
    "    benchmark_pair_ids = set(strategy_universe.get_pair_by_human_description(desc).internal_id for desc in SUPPORTING_PAIRS)\n",
    "\n",
    "    volatility_series = dependency_resolver.get_indicator_data(\n",
    "        volatility_inclusion_criteria,\n",
    "        parameters={\"rebalance_volalitity_bars\": rebalance_volalitity_bars},\n",
    "    )\n",
    "\n",
    "    volume_series = dependency_resolver.get_indicator_data(\n",
    "        volume_inclusion_criteria,\n",
    "        parameters={\n",
    "            \"min_volume\": min_volume,\n",
    "            \"rolling_volume_bars\": rolling_volume_bars,\n",
    "        },\n",
    "    )\n",
    "\n",
    "    tvl_series = dependency_resolver.get_indicator_data(\n",
    "        tvl_inclusion_criteria,\n",
    "        parameters={\n",
    "            \"min_tvl\": min_tvl,\n",
    "        },\n",
    "    )\n",
    "\n",
    "    trading_availability_series = dependency_resolver.get_indicator_data(trading_availability_criteria)\n",
    "\n",
    "    #\n",
    "    # Process all pair ids as a set and the final inclusion\n",
    "    # criteria is union of all sub-criterias\n",
    "    #\n",
    "\n",
    "    df = pd.DataFrame({\n",
    "        \"tvl_pair_ids\": tvl_series,\n",
    "        \"volume_pair_ids\": volume_series,\n",
    "        \"volatility_pair_ids\": volatility_series,\n",
    "        \"trading_availability_pair_ids\": trading_availability_series,\n",
    "    })\n",
    "\n",
    "    # https://stackoverflow.com/questions/33199193/how-to-fill-dataframe-nan-values-with-empty-list-in-pandas\n",
    "    df = df.fillna(\"\").apply(list)\n",
    "\n",
    "    # Volatility criteria removed so we get truly equally weighted index\n",
    "    # def _combine_criteria(row):\n",
    "    #     final_set = set(row[\"volume_pair_ids\"]) & set(row[\"volatility_pair_ids\"]) & set(row[\"tvl_pair_ids\"])\n",
    "    #     return final_set - benchmark_pair_ids\n",
    "\n",
    "    def _combine_criteria(row):\n",
    "        final_set = set(row[\"volume_pair_ids\"]) & \\\n",
    "                    set(row[\"tvl_pair_ids\"]) & \\\n",
    "                    set(row[\"trading_availability_pair_ids\"])\n",
    "        return final_set - benchmark_pair_ids\n",
    "\n",
    "    union_criteria = df.apply(_combine_criteria, axis=1)\n",
    "\n",
    "    # Inclusion criteria data can be spotty at the beginning when there is only 0 or 1 pairs trading,\n",
    "    # so we need to fill gaps to 0\n",
    "    full_index = pd.date_range(\n",
    "        start=union_criteria.index.min(),\n",
    "        end=union_criteria.index.max(),\n",
    "        freq=Parameters.candle_time_bucket.to_frequency(),\n",
    "    )\n",
    "    reindexed = union_criteria.reindex(full_index, fill_value=[])\n",
    "    return reindexed\n",
    "\n",
    "\n",
    "@indicators.define(dependencies=(volume_inclusion_criteria,), source=IndicatorSource.dependencies_only_universe)\n",
    "def volume_included_pair_count(\n",
    "    min_volume: USDollarAmount,\n",
    "    rolling_volume_bars: int,\n",
    "    dependency_resolver: IndicatorDependencyResolver\n",
    ") -> pd.Series:\n",
    "    series = dependency_resolver.get_indicator_data(\n",
    "        volume_inclusion_criteria,\n",
    "        parameters={\"min_volume\": min_volume, \"rolling_volume_bars\": rolling_volume_bars},\n",
    "    )\n",
    "    return series.apply(len)\n",
    "\n",
    "\n",
    "@indicators.define(dependencies=(volatility_inclusion_criteria,), source=IndicatorSource.dependencies_only_universe)\n",
    "def volatility_included_pair_count(\n",
    "    rebalance_volalitity_bars: int,\n",
    "    dependency_resolver: IndicatorDependencyResolver\n",
    ") -> pd.Series:\n",
    "    \"\"\"Calculate number of pairs in meeting volatility criteria on each timestamp\"\"\"\n",
    "    series = dependency_resolver.get_indicator_data(\n",
    "        volatility_inclusion_criteria,\n",
    "        parameters={\"rebalance_volalitity_bars\": rebalance_volalitity_bars},\n",
    "    )\n",
    "    return series.apply(len)\n",
    "\n",
    "\n",
    "@indicators.define(dependencies=(tvl_inclusion_criteria,), source=IndicatorSource.dependencies_only_universe)\n",
    "def tvl_included_pair_count(\n",
    "        min_tvl: USDollarAmount,\n",
    "        dependency_resolver: IndicatorDependencyResolver\n",
    ") -> pd.Series:\n",
    "    \"\"\"Calculate number of pairs in meeting volatility criteria on each timestamp\"\"\"\n",
    "    series = dependency_resolver.get_indicator_data(\n",
    "        tvl_inclusion_criteria,\n",
    "        parameters={\"min_tvl\": min_tvl},\n",
    "    )\n",
    "    series = series.apply(len)\n",
    "\n",
    "    # TVL data can be spotty at the beginning when there is only 0 or 1 pairs trading,\n",
    "    # so we need to fill gaps to 0\n",
    "    full_index = pd.date_range(\n",
    "        start=series.index.min(),\n",
    "        end=series.index.max(),\n",
    "        freq=Parameters.candle_time_bucket.to_frequency(),\n",
    "    )\n",
    "    # Reindex and fill NaN with zeros\n",
    "    reindexed = series.reindex(full_index, fill_value=0)\n",
    "    return reindexed\n",
    "\n",
    "\n",
    "\n",
    "@indicators.define(dependencies=(inclusion_criteria,), source=IndicatorSource.dependencies_only_universe)\n",
    "def all_criteria_included_pair_count(\n",
    "    min_volume: USDollarAmount,\n",
    "    min_tvl: USDollarAmount,\n",
    "    rolling_volume_bars: int,\n",
    "    rebalance_volalitity_bars: int,\n",
    "    dependency_resolver: IndicatorDependencyResolver\n",
    ") -> pd.Series:\n",
    "    \"\"\"Series where each timestamp is the list of pairs meeting all inclusion criteria.\n",
    "\n",
    "    :return:\n",
    "        Series with pair count for each timestamp\n",
    "    \"\"\"\n",
    "    series = dependency_resolver.get_indicator_data(\n",
    "        \"inclusion_criteria\",\n",
    "        parameters={\n",
    "            \"min_volume\": min_volume, \n",
    "            \"min_tvl\": min_tvl, \n",
    "            \"rolling_volume_bars\": rolling_volume_bars,\n",
    "            \"rebalance_volalitity_bars\": rebalance_volalitity_bars,\n",
    "        },\n",
    "    )\n",
    "    return series.apply(len)\n",
    "\n",
    "\n",
    "@indicators.define(source=IndicatorSource.strategy_universe)\n",
    "def trading_pair_count(\n",
    "    strategy_universe: TradingStrategyUniverse,\n",
    ") -> pd.Series:\n",
    "    \"\"\"Get number of pairs that trade at each timestamp.\n",
    "\n",
    "    - Pair must have had at least one candle before the timestamp to be included\n",
    "\n",
    "    - Exclude benchmarks pairs we do not trade\n",
    "\n",
    "    :return:\n",
    "        Series with pair count for each timestamp\n",
    "    \"\"\"\n",
    "\n",
    "    benchmark_pair_ids = {strategy_universe.get_pair_by_human_description(desc).internal_id for desc in SUPPORTING_PAIRS}\n",
    "\n",
    "    # Get pair_id, timestamp -> timestamp, pair_id index\n",
    "    series = strategy_universe.data_universe.candles.df[\"open\"]    \n",
    "    swap_index = series.index.swaplevel(0, 1)\n",
    "\n",
    "    seen_pairs = set()\n",
    "    seen_data = {}\n",
    "\n",
    "    for timestamp, pair_id in swap_index:\n",
    "        if pair_id in benchmark_pair_ids:\n",
    "            continue\n",
    "        seen_pairs.add(pair_id)\n",
    "        seen_data [timestamp] = len(seen_pairs)\n",
    "\n",
    "    series = pd.Series(seen_data.values(), index=list(seen_data.keys()))\n",
    "    return series\n",
    "\n",
    "\n",
    "@indicators.define(dependencies=(volatility_ewm,), source=IndicatorSource.strategy_universe)\n",
    "def volume_weighted_avg_volatility(\n",
    "    strategy_universe,\n",
    "    rebalance_volalitity_bars: int,\n",
    "    dependency_resolver: IndicatorDependencyResolver\n",
    ") -> pd.Series:\n",
    "    \"\"\"Calculate the volume-weighted volatility for the whole index.\n",
    "\n",
    "    This does not make really sense, but we calculate anyway.\n",
    "\n",
    "    :return:\n",
    "        Series with pair count for each timestamp\n",
    "    \"\"\"\n",
    "\n",
    "    volatility_series = dependency_resolver.get_indicator_data_pairs_combined(\n",
    "        volatility_ewm,\n",
    "        parameters={\"rebalance_volalitity_bars\": rebalance_volalitity_bars},\n",
    "    )\n",
    "\n",
    "    volume_series = strategy_universe.data_universe.candles.df[\"volume\"]\n",
    "\n",
    "    # Create DataFrames for easier manipulation\n",
    "    df = pd.DataFrame({\n",
    "        'volatility': volatility_series,\n",
    "        'volume': volume_series\n",
    "    })\n",
    "        \n",
    "    df['weighted'] = df['volatility'] * df['volume']\n",
    "\n",
    "    # Group by timestamp and calculate weighted average\n",
    "    grouped = df.groupby(level='timestamp')\n",
    "    \n",
    "    volume_weighted_volatility = (\n",
    "        grouped['weighted'].sum() / grouped['volume'].sum()\n",
    "    )\n",
    "    \n",
    "    return volume_weighted_volatility\n",
    "\n",
    "\n",
    "@indicators.define(dependencies=(volatility_ewm,), source=IndicatorSource.strategy_universe)\n",
    "def avg_volatility(\n",
    "    strategy_universe,\n",
    "    rebalance_volalitity_bars: int,\n",
    "    dependency_resolver: IndicatorDependencyResolver\n",
    ") -> pd.Series:\n",
    "    \"\"\"Calculate index avg volatility across all trading pairs.\n",
    "\n",
    "    :return:\n",
    "        Series with pair count for each timestamp\n",
    "    \"\"\"\n",
    "\n",
    "    volatility = dependency_resolver.get_indicator_data_pairs_combined(\n",
    "        \"volatility\",\n",
    "        parameters={\"rebalance_volalitity_bars\": rebalance_volalitity_bars},\n",
    "    )\n",
    "\n",
    "    n_std = 3\n",
    "    def remove_outliers_group(group):\n",
    "        mean = group.mean()\n",
    "        std = group.std()\n",
    "        lower_bound = mean - n_std * std\n",
    "        upper_bound = mean + n_std * std\n",
    "        return group[(group >= lower_bound) & (group <= upper_bound)]\n",
    "    \n",
    "    cleaned = volatility.groupby(level='timestamp').apply(remove_outliers_group)\n",
    "    \n",
    "    # Group by timestamp, remove outliers within each group, then calculate mean\n",
    "    cleaned_volatility = cleaned.groupby(level=0).mean()\n",
    "    \n",
    "    return cleaned_volatility\n",
    "\n",
    "\n",
    "@indicators.define(dependencies=(volatility_returns_indicator,), source=IndicatorSource.dependencies_only_universe)\n",
    "def avg_signal(\n",
    "    rebalance_volalitity_bars: int,\n",
    "    returns_bars: int,\n",
    "    dependency_resolver: IndicatorDependencyResolver\n",
    ") -> pd.Series:\n",
    "    \"\"\"Calculate our \"signal\" across all pairs\n",
    "\n",
    "    :return:\n",
    "        Series with pair count for each timestamp\n",
    "    \"\"\"\n",
    "\n",
    "    signal = dependency_resolver.get_indicator_data_pairs_combined(\n",
    "        volatility_returns_indicator,\n",
    "        parameters={\n",
    "            \"rebalance_volalitity_bars\": rebalance_volalitity_bars,\n",
    "            \"returns_bars\": returns_bars,\n",
    "        },\n",
    "    )\n",
    "\n",
    "    n_std = 3\n",
    "    def remove_outliers_group(group):\n",
    "        mean = group.mean()\n",
    "        std = group.std()\n",
    "        lower_bound = mean - n_std * std\n",
    "        upper_bound = mean + n_std * std\n",
    "        return group[(group >= lower_bound) & (group <= upper_bound)]\n",
    "    \n",
    "    cleaned = signal.groupby(level='timestamp').apply(remove_outliers_group)\n",
    "    \n",
    "    # Group by timestamp, remove outliers within each group, then calculate mean\n",
    "    cleaned_avg_signal = cleaned.groupby(level=0).mean()\n",
    "    \n",
    "    return cleaned_avg_signal\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backtest time range\n",
    "\n",
    "- Choose the backtesting time range\n",
    "- Start when we have enough assets (`Parameters.min_asset_universe`) in our asset universe to form the first basket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the first date where the condition is True\n",
    "backtest_start = Parameters.backtest_start\n",
    "backtest_end = Parameters.backtest_end\n",
    "\n",
    "print(f\"Time range is {backtest_start} - {backtest_end}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Strategy algorithm and backtest\n",
    "\n",
    "- Run the backtest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tradeexecutor.backtest.backtest_runner import run_backtest_inline\n",
    "from tradeexecutor.strategy.alpha_model import AlphaModel\n",
    "from tradeexecutor.state.trade import TradeExecution\n",
    "from tradeexecutor.strategy.pandas_trader.strategy_input import StrategyInput, IndicatorDataNotFoundWithinDataTolerance\n",
    "from tradeexecutor.state.visualisation import PlotKind\n",
    "from tradeexecutor.backtest.backtest_runner import run_backtest_inline\n",
    "from tradeexecutor.strategy.tvl_size_risk import USDTVLSizeRiskModel\n",
    "from tradeexecutor.strategy.weighting import weight_by_1_slash_n, weight_passthrouh, weight_equal\n",
    "from tradeexecutor.utils.dedent import dedent_any\n",
    "from tradeexecutor.strategy.parameters import RollingParameter\n",
    "\n",
    "\n",
    "\n",
    "def decide_trades(\n",
    "    input: StrategyInput\n",
    ") -> list[TradeExecution]:\n",
    "    \"\"\"For each strategy tick, generate the list of trades.\"\"\"\n",
    "    parameters = input.parameters\n",
    "    position_manager = input.get_position_manager()\n",
    "    state = input.state\n",
    "    timestamp = input.timestamp\n",
    "    indicators = input.indicators\n",
    "    strategy_universe = input.strategy_universe\n",
    "\n",
    "    # Build signals for each pair \n",
    "    alpha_model = AlphaModel(\n",
    "        timestamp,\n",
    "        close_position_weight_epsilon=parameters.min_portfolio_weight,  # 10 BPS is our min portfolio weight\n",
    "    )\n",
    "\n",
    "    # Prepare diagnostics variables\n",
    "    max_vol = (0, None)  \n",
    "    signal_count = 0  \n",
    "    clipped_pairs = 0\n",
    "\n",
    "    # Resolve rolling parameters\n",
    "    varying = isinstance(parameters[\"returns_bars\"], RollingParameter)\n",
    "    min_signal_threshold = parameters.get_rolling_parameter(\"min_signal_threshold\", timestamp)\n",
    "    rebalance_volalitity_bars = parameters.get_rolling_parameter(\"rebalance_volalitity_bars\", timestamp)\n",
    "    returns_bars = parameters.get_rolling_parameter(\"returns_bars\", timestamp)\n",
    "\n",
    "    if varying:\n",
    "        # We run a backtest were parameters vary over time\n",
    "        included_pairs = indicators.get_indicator_value(\n",
    "            \"inclusion_criteria\",\n",
    "            na_conversion=False,\n",
    "            parameters={\n",
    "                \"rebalance_volalitity_bars\": rebalance_volalitity_bars,\n",
    "                \"returns_bars\": returns_bars,\n",
    "            }\n",
    "        )    \n",
    "    else:\n",
    "        # Path taken by normal backtest/grid search\n",
    "        included_pairs = indicators.get_indicator_value(\n",
    "            \"inclusion_criteria\",\n",
    "            na_conversion=False,\n",
    "        )    \n",
    "\n",
    "    if included_pairs is None:\n",
    "        included_pairs = []\n",
    "\n",
    "    # Set signal for each pair\n",
    "    for pair_id in included_pairs:\n",
    "        pair = strategy_universe.get_pair_by_id(pair_id)\n",
    "\n",
    "        if varying:\n",
    "            pair_signal = indicators.get_indicator_value(\n",
    "                \"volatility_returns_indicator\", \n",
    "                pair=pair,\n",
    "                parameters={\n",
    "                    \"rebalance_volalitity_bars\": rebalance_volalitity_bars,\n",
    "                    \"returns_bars\": returns_bars,\n",
    "                }                \n",
    "            )\n",
    "\n",
    "        else:\n",
    "            pair_signal = indicators.get_indicator_value(\n",
    "                \"volatility_returns_indicator\", \n",
    "                pair=pair\n",
    "            )\n",
    "        if pair_signal is None:\n",
    "            continue\n",
    "        \n",
    "        weight = pair_signal - avg_signal - min_signal_threshold\n",
    "\n",
    "        if weight < 0:\n",
    "            continue\n",
    "    \n",
    "        alpha_model.set_signal(\n",
    "            pair,\n",
    "            weight,\n",
    "        )\n",
    "\n",
    "        # Diagnostics reporting\n",
    "        signal_count += 1\n",
    "\n",
    "\n",
    "    # Calculate how much dollar value we want each individual position to be on this strategy cycle,\n",
    "    # based on our total available equity\n",
    "    portfolio = position_manager.get_current_portfolio()\n",
    "    portfolio_target_value = portfolio.get_total_equity() * parameters.allocation\n",
    "\n",
    "    # Select max_assets_in_portfolio assets in which we are going to invest\n",
    "    # Calculate a weight for ecah asset in the portfolio using 1/N method based on the raw signal\n",
    "    alpha_model.select_top_signals(count=parameters.max_assets_in_portfolio)\n",
    "    alpha_model.assign_weights(method=weight_passthrouh)\n",
    "    # alpha_model.assign_weights(method=weight_by_1_slash_n)\n",
    "\n",
    "    #\n",
    "    # Normalise weights and cap the positions\n",
    "    # \n",
    "    size_risk_model = USDTVLSizeRiskModel(\n",
    "        pricing_model=input.pricing_model,\n",
    "        per_position_cap=parameters.per_position_cap_of_pool,  # This is how much % by all pool TVL we can allocate for a position\n",
    "        missing_tvl_placeholder_usd=parameters.assummed_liquidity_when_data_missings,  # Placeholder for missing TVL data until we get the data off the chain\n",
    "    )\n",
    "\n",
    "    alpha_model.normalise_weights(\n",
    "        investable_equity=portfolio_target_value,\n",
    "        size_risk_model=size_risk_model,\n",
    "        max_weight=parameters.max_concentration,\n",
    "    )\n",
    "\n",
    "    # Load in old weight for each trading pair signal,\n",
    "    # so we can calculate the adjustment trade size\n",
    "    alpha_model.update_old_weights(\n",
    "        state.portfolio,\n",
    "        ignore_credit=True,\n",
    "    )\n",
    "    alpha_model.calculate_target_positions(position_manager)\n",
    "\n",
    "    # Shift portfolio from current positions to target positions\n",
    "    # determined by the alpha signals (momentum)\n",
    "    \n",
    "    # rebalance_threshold_usd = portfolio_target_value * parameters.min_rebalance_trade_threshold_pct\n",
    "    rebalance_threshold_usd = parameters.individual_rebalance_min_threshold_usd\n",
    "    \n",
    "    assert rebalance_threshold_usd > 0.1, \"Safety check tripped - something like wrong with strat code\"\n",
    "    trades = alpha_model.generate_rebalance_trades_and_triggers(\n",
    "        position_manager,\n",
    "        min_trade_threshold=rebalance_threshold_usd,  # Don't bother with trades under XXXX USD\n",
    "        invidiual_rebalance_min_threshold=parameters.individual_rebalance_min_threshold_usd,\n",
    "        execution_context=input.execution_context,\n",
    "    )\n",
    "\n",
    "    # Supply or withdraw cash to Aave if strategy is set to do so\n",
    "    if parameters.use_aave:\n",
    "        credit_deposit_flow = position_manager.calculate_credit_flow_needed(\n",
    "            trades,\n",
    "            parameters.allocation,\n",
    "        )\n",
    "        trades += position_manager.manage_credit_flow(credit_deposit_flow)\n",
    "    else:\n",
    "        credit_deposit_flow = 0\n",
    "\n",
    "    # Add verbal report about decision made/not made,\n",
    "    # so it is much easier to diagnose live trade execution.\n",
    "    # This will be readable in Discord/Telegram logging.\n",
    "    if input.is_visualisation_enabled():\n",
    "        vol_pair = strategy_universe.get_pair_by_human_description(VOL_PAIR)\n",
    "        volume_included_pair_count = indicators.get_indicator_value(\n",
    "            \"volume_included_pair_count\",\n",
    "        )\n",
    "        volatility_included_pair_count = indicators.get_indicator_value(\n",
    "            \"volatility_included_pair_count\",\n",
    "        ) \n",
    "        tvl_included_pair_count = indicators.get_indicator_value(\n",
    "            \"tvl_included_pair_count\",\n",
    "        )        \n",
    "        try:\n",
    "            top_signal = next(iter(alpha_model.get_signals_sorted_by_weight()))\n",
    "            if top_signal.normalised_weight == 0:\n",
    "                top_signal = None\n",
    "        except StopIteration:\n",
    "            top_signal = None\n",
    "\n",
    "        max_vol_pair = max_vol[1]\n",
    "        if max_vol_pair:\n",
    "            max_vol_signal = alpha_model.get_signal_by_pair(max_vol_pair)\n",
    "        else:\n",
    "            max_vol_signal = None\n",
    "\n",
    "        vol_pair_vol = indicators.get_indicator_value(\"volatility_ewm\", pair=vol_pair)\n",
    "\n",
    "        rebalance_volume = sum(t.get_value() for t in trades)\n",
    "        \n",
    "        report = dedent_any(f\"\"\"\n",
    "        Cycle: #{input.cycle}\n",
    "        Rebalanced: {'👍' if alpha_model.is_rebalance_triggered() else '👎'}\n",
    "        Open/about to open positions: {len(state.portfolio.open_positions)} \n",
    "        Max position value change: {alpha_model.max_position_adjust_usd:,.2f} USD\n",
    "        Rebalance threshold: {alpha_model.position_adjust_threshold_usd:,.2f} USD\n",
    "        Trades decided: {len(trades)}\n",
    "        Pairs total: {strategy_universe.data_universe.pairs.get_count()}\n",
    "        Pairs meeting inclusion criteria: {len(included_pairs)}\n",
    "        Pairs meeting volume inclusion criteria: {volume_included_pair_count}\n",
    "        Pairs meeting volatility inclusion criteria: {volatility_included_pair_count}        \n",
    "        Pairs meeting TVL inclusion criteria: {tvl_included_pair_count}        \n",
    "        Pairs under volatility threshold: {clipped_pairs}        \n",
    "        Signals created: {signal_count}\n",
    "        Total equity: {portfolio.get_total_equity():,.2f} USD\n",
    "        Cash: {position_manager.get_current_cash():,.2f} USD\n",
    "        Investable equity: {alpha_model.investable_equity:,.2f} USD\n",
    "        Accepted investable equity: {alpha_model.accepted_investable_equity:,.2f} USD\n",
    "        Allocated to signals: {alpha_model.get_allocated_value():,.2f} USD\n",
    "        Discarted allocation because of lack of lit liquidity: {alpha_model.size_risk_discarded_value:,.2f} USD\n",
    "        Credit deposit flow: {credit_deposit_flow:,.2f} USD\n",
    "        Rebalance volume: {rebalance_volume:,.2f} USD\n",
    "        {vol_pair.base.token_symbol} volatility: {vol_pair_vol}        \n",
    "        Most volatility pair: {max_vol_pair.get_ticker() if max_vol_pair else '-'}\n",
    "        Most volatility pair vol: {max_vol[0]}\n",
    "        Most volatility pair signal value: {max_vol_signal.signal if max_vol_signal else '-'}\n",
    "        Most volatility pair signal weight: {max_vol_signal.raw_weight if max_vol_signal else '-'}        \n",
    "        \"\"\")\n",
    "\n",
    "        # Most volatility pair signal weight (normalised): {max_vol_signal.normalised_weight * 100 if max_vol_signal else '-'} % (got {max_vol_signal.position_size_risk.get_relative_capped_amount() * 100 if max_vol_signal else '-'} % of asked size)\n",
    "        if top_signal:\n",
    "            top_signal_vol = indicators.get_indicator_value(\"volatility_ewm\", pair=top_signal.pair)\n",
    "            assert top_signal.position_size_risk\n",
    "            report += dedent_any(f\"\"\"\n",
    "            Top signal pair: {top_signal.pair.get_ticker()}\n",
    "            Top signal volatility: {top_signal_vol}\n",
    "            Top signal value: {top_signal.signal}\n",
    "            Top signal weight: {top_signal.raw_weight}\n",
    "            Top signal weight (normalised): {top_signal.normalised_weight * 100:.2f} % (got {top_signal.position_size_risk.get_relative_capped_amount() * 100:.2f} % of asked size)\n",
    "            \"\"\")            \n",
    "\n",
    "        for flag, count in alpha_model.get_flag_diagnostics_data().items():\n",
    "            report += f\"Signals with flag {flag.name}: {count}\\n\"\n",
    "\n",
    "        state.visualisation.add_message(\n",
    "            timestamp,\n",
    "            report, \n",
    "        )\n",
    "\n",
    "        state.visualisation.set_discardable_data(\"alpha_model\", alpha_model)\n",
    "    \n",
    "    return trades  # Return the list of trades we made in this cycle\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Periods\n",
    "\n",
    "- Define what periods we use for the optimiser\n",
    "- Each period looks backs X months of history for bactesting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "optimiser_periods_start = Parameters.backtest_start + pd.offsets.MonthBegin(1)\n",
    "optimiser_periods_end = Parameters.backtest_end - pd.offsets.MonthBegin(1)\n",
    "\n",
    "month_begin_dates = pd.date_range(start=optimiser_periods_start, end=optimiser_periods_end, freq='MS')\n",
    "\n",
    "# Drop the last incomplete period\n",
    "optimiser_periods = month_begin_dates[0:-2]\n",
    "\n",
    "print(f\"Total {len(optimiser_periods)} periods\")\n",
    "for p in optimiser_periods:\n",
    "    print(p)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run periodic optimiser loop\n",
    "\n",
    "- Run optimiser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from tradeexecutor.backtest.optimiser import perform_optimisation\n",
    "from tradeexecutor.backtest.optimiser import prepare_optimiser_parameters\n",
    "from tradeexecutor.backtest.optimiser import MinTradeCountFilter\n",
    "from tradeexecutor.backtest.optimiser_functions import optimise_sharpe, optimise_sortino\n",
    "\n",
    "# How many Gaussian Process iterations we do\n",
    "iterations = 3\n",
    "\n",
    "# What do we optimise for\n",
    "# search_func = BalancedSharpeAndMaxDrawdownOptimisationFunction(sharpe_weight=0.75, max_drawdown_weight=0.25)\n",
    "search_func = optimise_sortino\n",
    "\n",
    "optimiser_base_id = Parameters.id\n",
    "\n",
    "optimiser_results = []\n",
    "optimiser_checkpoint_period = Parameters.optimiser_checkpoint_period\n",
    "\n",
    "# Period marks the beginning of 1 month run with the optimised parameters\n",
    "for index, period in enumerate(optimiser_periods, start=1):\n",
    "\n",
    "    last = (index == len(optimiser_periods))\n",
    "\n",
    "    parameters = prepare_optimiser_parameters(Parameters)\n",
    "    parameters[\"backtest_start\"] = period - parameters[\"optimiser_lookback_period\"]\n",
    "    parameters[\"backtest_end\"] = period \n",
    "    parameters[\"id\"] = f\"{optimiser_base_id}-{index}-{period.strftime('%Y-%m')}\"\n",
    "    parameters[\"optimiser_period_id\"] = index\n",
    "\n",
    "    start = datetime.datetime.utcnow()\n",
    "\n",
    "    print(f\"Starting optimiser cycle {index}/{len(optimiser_periods)}: {parameters['backtest_start']} - {parameters['backtest_end']}\")\n",
    "\n",
    "    optimiser_result = perform_optimisation(\n",
    "        iterations=iterations,\n",
    "        search_func=search_func,\n",
    "        decide_trades=decide_trades,\n",
    "        strategy_universe=strategy_universe,\n",
    "        parameters=parameters,  \n",
    "        create_indicators=indicators.create_indicators,\n",
    "        result_filter=MinTradeCountFilter(Parameters.min_trade_count),\n",
    "        timeout=20*60,    \n",
    "        ignore_wallet_errors=True,  \n",
    "        # Uncomment for diagnostics\n",
    "        # log_level=logging.INFO,\n",
    "        # max_workers=1,\n",
    "    )\n",
    "\n",
    "    optimiser_results.append(optimiser_result)\n",
    "    best = optimiser_result.find_best(sort_key=lambda result: result.result.get_sortino())\n",
    "\n",
    "    duration = datetime.datetime.utcnow() - start\n",
    "    print(f\"Optimiser period {index}/{len(optimiser_periods)} completed in {duration}\")\n",
    "    print(f\"Best peformance:\", best.result)\n",
    "    print(f\"Backtesting time for the best result {best.result.backtest_start} - {best.result.backtest_end}\")\n",
    "    print(f\"First trade at {best.result.first_trade_at}\")\n",
    "    print(f\"Optimise completed, optimiser searched {optimiser_result.get_combination_count()} combinations, with {optimiser_result.get_cached_count()} results read directly from cache. and {optimiser_result.get_filtered_count()} filtered results.\")\n",
    "    print(f\"Backtests failed with exception:\", optimiser_result.get_failed_count())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Examine tuned parameters\n",
    "\n",
    "- Show chosen tuned parameters for each tuning period"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from tradeexecutor.backtest.grid_search import GridSearchResult, GridParameter\n",
    "\n",
    "data = []\n",
    "dynamic_parameters = defaultdict(list)\n",
    "index = []\n",
    "\n",
    "for optimiser_result in optimiser_results:\n",
    "    best = optimiser_result.find_best(sort_key=lambda result: result.result.get_sortino())\n",
    "    entry = {\n",
    "        \"start\": optimiser_result.parameters[\"backtest_start\"],\n",
    "        \"end\": optimiser_result.parameters[\"backtest_end\"],\n",
    "        \"checkpoint\": optimiser_result.parameters[\"backtest_end\"],\n",
    "    }\n",
    "\n",
    "    index.append(optimiser_result.parameters[\"backtest_end\"])\n",
    "    \n",
    "    parameter: GridParameter\n",
    "    for parameter in best.result.combination.parameters:\n",
    "        entry[parameter.name] = parameter.value\n",
    "\n",
    "    for parameter in best.result.combination.parameters:\n",
    "        dynamic_parameters[parameter.name].append(parameter.value)\n",
    "\n",
    "    entry[\"cagr\"] = best.result.get_cagr()\n",
    "    entry[\"sharpe\"] = best.result.get_sharpe()\n",
    "    entry[\"sortino\"] = best.result.get_sortino()\n",
    "    entry[\"max_drawdown\"] = best.result.get_max_drawdown()\n",
    "    data.append(entry)\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "df = df.set_index(\"checkpoint\")\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualise parameter change over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# Create subplots with shared X-axis and multiple Y-axes\n",
    "fig = make_subplots(specs=[[{\"secondary_y\": True}]])\n",
    "\n",
    "parameter_names = list(dynamic_parameters.keys())\n",
    "\n",
    "# Add traces for each column with its own Y-axis\n",
    "for i, col in enumerate(parameter_names):  # Skip the 'x' column\n",
    "    fig.add_trace(\n",
    "        go.Scatter(x=df.index, y=df[col], name=col),\n",
    "        secondary_y=(i > 0)  # Use secondary Y-axis for all columns except the first\n",
    "    )\n",
    "\n",
    "\n",
    "# Update layout for better visualization\n",
    "fig.update_layout(\n",
    "    title=\"Multiline Chart with Separate Y-Axes\",\n",
    "    #margin=dict(l=510),\n",
    "    )\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up dynamic parameters\n",
    "\n",
    "- Each dynamic parameter is a dict of {start_date: value}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tradeexecutor.backtest.grid_search import GridSearchResult, GridParameter\n",
    "from tradeexecutor.strategy.parameters import RollingParameter\n",
    "from tradeexecutor.strategy.parameters import display_parameters\n",
    "\n",
    "# Construct RollingParameter instances\n",
    "index = pd.DatetimeIndex(index)\n",
    "\n",
    "print(f\"Dynamic parameters are {len(dynamic_parameters)}:\")\n",
    "parameters = StrategyParameters.from_class(Parameters)\n",
    "for key, values in dynamic_parameters.items():\n",
    "    rolling_p = RollingParameter(\n",
    "        name=key,\n",
    "        freq=Parameters.optimiser_checkpoint_period,\n",
    "        values=pd.Series(values, index=index)\n",
    "    )\n",
    "    parameters[key] = rolling_p\n",
    "    print(rolling_p)\n",
    "\n",
    "display_parameters(parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from pprint import pprint\n",
    "\n",
    "from tradeexecutor.strategy.execution_context import notebook_execution_context\n",
    "from tradeexecutor.strategy.pandas_trader.indicator import prepare_indicators\n",
    "\n",
    "\n",
    "logging.getLogger().setLevel(logging.INFO)\n",
    "\n",
    "\n",
    "indicator_set = prepare_indicators(\n",
    "    indicators.create_indicators, \n",
    "    parameters, \n",
    "    strategy_universe, \n",
    "    notebook_execution_context, \n",
    ")\n",
    "\n",
    "for x in indicator_set.indicators.items():\n",
    "    print(x)\n",
    "\n",
    "df = display_parameters(parameters)\n",
    "display(df)\n",
    "\n",
    "all_combinations = indicator_set.generate_combinations(strategy_universe)\n",
    "for combination in all_combinations:    \n",
    "    for key, value in combination.definition.parameters.items():\n",
    "        assert not isinstance(value, list), f\"Bad key: {key}, value: {value}, {value.__class__}, {combination.definition}\"\n",
    "        assert not isinstance(value, RollingParameter), f\"RollingParameter not expanded: {key}, value: {value}, {value.__class__}, {combination.definition}\"\n",
    "    # print(combination)\n",
    "\n",
    "logging.getLogger().setLevel(logging.WARNING)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run backtest with dynamic parameters\n",
    "\n",
    "- Do a single backtest run\n",
    "- Use dynamically adjusted parameters for each "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = run_backtest_inline(\n",
    "    name=parameters.id,\n",
    "    engine_version=\"0.5\",\n",
    "    decide_trades=decide_trades,\n",
    "    create_indicators=indicators.create_indicators,\n",
    "    client=client,\n",
    "    universe=strategy_universe,\n",
    "    parameters=parameters,\n",
    "    # log_level=logging.INFO,\n",
    "    max_workers=1,\n",
    "    start_at=backtest_start,\n",
    "    end_at=backtest_end,\n",
    ")\n",
    "\n",
    "state = result.state\n",
    "\n",
    "trade_count = len(list(state.portfolio.get_all_trades()))\n",
    "print(f\"Backtesting completed, backtested strategy made {trade_count} trades\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performance metrics\n",
    "\n",
    "- Display portfolio performance metrics\n",
    "- Compare against buy and hold matic using the same initial capital\n",
    "\n",
    "**Note**: Some of these metrics might be incorrect due to slow start of the strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tradeexecutor.analysis.multi_asset_benchmark import compare_strategy_backtest_to_multiple_assets\n",
    "\n",
    "compare_strategy_backtest_to_multiple_assets(\n",
    "    state,\n",
    "    strategy_universe,\n",
    "    display=True,\n",
    "    interesting_assets=EXAMINED_ASSETS,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Equity curve\n",
    "\n",
    "- Equity curve shows how your strategy accrues value over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tradeexecutor.analysis.multi_asset_benchmark import get_benchmark_data\n",
    "from tradeexecutor.visual.benchmark import visualise_equity_curve_benchmark\n",
    "\n",
    "benchmark_indexes = get_benchmark_data(\n",
    "    strategy_universe,\n",
    "    cumulative_with_initial_cash=state.portfolio.get_initial_cash(),\n",
    "    max_count=4,\n",
    "    start_at=state.get_trading_time_range()[0],\n",
    "    interesting_assets=[\"WETH\", \"ANDY\", \"weirdo\"],\n",
    ")\n",
    "\n",
    "fig = visualise_equity_curve_benchmark(\n",
    "    state=state,    \n",
    "    benchmark_indexes=benchmark_indexes,\n",
    "    height=800,\n",
    "    log_y=True,\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Equity curve with drawdown\n",
    "\n",
    "- Linear curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tradeexecutor.visual.equity_curve import calculate_equity_curve, calculate_returns\n",
    "from tradeexecutor.visual.equity_curve import visualise_equity_curve\n",
    "\n",
    "curve = calculate_equity_curve(state)\n",
    "returns = calculate_returns(curve)\n",
    "fig = visualise_equity_curve(returns)\n",
    "display(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Asset weights\n",
    "\n",
    "- What assets were allocated over time\n",
    "- Do both proportional % and USD weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tradeexecutor.analysis.weights import calculate_asset_weights, visualise_weights\n",
    "\n",
    "weights_series = calculate_asset_weights(state)\n",
    "\n",
    "fig = visualise_weights(\n",
    "    weights_series,\n",
    "    normalised=True,\n",
    "    include_reserves=False,\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fig = visualise_weights(\n",
    "    weights_series,\n",
    "    normalised=True,\n",
    "    include_reserves=True,\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = weights_series.unstack(level=1)\n",
    "\n",
    "fig = visualise_weights(\n",
    "    weights_series,\n",
    "    normalised=False,\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weight allocation statistics\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tradeexecutor.analysis.weights import calculate_weights_statistics\n",
    "\n",
    "stats = calculate_weights_statistics(weights_series)\n",
    "display(stats)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "trading-strategy-getting-started-CRdZaTBS-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
